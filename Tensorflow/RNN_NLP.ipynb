{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Movie Review Dataset\n",
        "We'll start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and  has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example. a word encoded by the integer 3 means that it is the 3rd most common word in the dataset"
      ],
      "metadata": {
        "id": "7Gh_whlez0m3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r8WWwToxu_nF"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words= VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at one review\n",
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITcYaBDN1SBB",
        "outputId": "e7b744a2-8c9d-4f18-e7c6-b0eab48c540b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data[1]))\n",
        "print(len(train_data[9]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x76NwChA291u",
        "outputId": "9e844234-97c4-4229-ddc8-01d31af024a7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189\n",
            "130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Preprocessing\n",
        "If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue, we cannot pass different length data into our neural network.Therefore, we must make each review the same length. To do this, we will follow the procedure below:\n",
        "* If the review is greater than 250 words then trim off the extra words\n",
        "* If the review is less than 250 words, add the necessary 0's to make it 250 words\n",
        "\n",
        "Luckily for us, keras has a function that can do this for us:\n",
        "\n"
      ],
      "metadata": {
        "id": "hC23nNZq1wKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "0SYbQEIJ2rKC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data[1]))\n",
        "print(len(train_data[9]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5tfPNOa5Nzg",
        "outputId": "4c726eef-8c66-41b8-ef2a-a287be077585"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250\n",
            "250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Model\n",
        "Now, its time to create the model. We'll use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.\n",
        "32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like."
      ],
      "metadata": {
        "id": "GH78x5uk3JX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# The reason we have used dense with the sigmoid activation function is:\n",
        "# We are trying to predict the sentiment of this.i.e if we have the sentiment btw 0 and 1\n",
        "# If for a data it is less than 0.5, we can classify as negative\n",
        "# Sigmoid activation function squishes our value btw 0 and 1"
      ],
      "metadata": {
        "id": "mx6-1go33nuC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9vbt6Ew3_DR",
        "outputId": "aedfd211-ea81-4a45-e21e-7c919793ad06"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2843041 (10.85 MB)\n",
            "Trainable params: 2843041 (10.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Now, its time to compile and train the model"
      ],
      "metadata": {
        "id": "HxMqyMhY62Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4g9FLte65ZR",
        "outputId": "5f1793a8-9d20-4623-cfb6-6467fc1f0bf0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 34s 51ms/step - loss: 0.4472 - acc: 0.7822 - val_loss: 0.3064 - val_acc: 0.8798\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.2588 - acc: 0.9004 - val_loss: 0.3321 - val_acc: 0.8584\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1987 - acc: 0.9273 - val_loss: 0.2927 - val_acc: 0.8846\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 8s 14ms/step - loss: 0.1632 - acc: 0.9423 - val_loss: 0.3749 - val_acc: 0.8624\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.1330 - acc: 0.9547 - val_loss: 0.3412 - val_acc: 0.8818\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.1163 - acc: 0.9613 - val_loss: 0.3275 - val_acc: 0.8810\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 0.0964 - acc: 0.9681 - val_loss: 0.3707 - val_acc: 0.8834\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 0.0783 - acc: 0.9736 - val_loss: 0.3713 - val_acc: 0.8620\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.0662 - acc: 0.9791 - val_loss: 0.4008 - val_acc: 0.8800\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0566 - acc: 0.9829 - val_loss: 0.4387 - val_acc: 0.8712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll evaluate the model on our training data to see how well it performs"
      ],
      "metadata": {
        "id": "38dir4oHWqxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOx3yenaW0cp",
        "outputId": "ac42bcfb-38f7-43fc-eeaa-3fe377098cc6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4878 - acc: 0.8610\n",
            "[0.4878470301628113, 0.8610000014305115]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we're scoring somewhere in the mid-high 80's. Not bad for a simple recurrent network."
      ],
      "metadata": {
        "id": "ieeAstaMXAad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions\n",
        "Now, let's use our network to make predictions on our own reviews.\n",
        "\n",
        "Since our reviews are encoded, we'll need to convert any review that we write into that form so the network can understand it. To do that, we'll load the encodings from the dataset adn use them to encode our own data."
      ],
      "metadata": {
        "id": "D49LPQXhXNQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = 'that movie was just amazing, so amazing'\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH6j6xCpX597",
        "outputId": "d32ffa8d-acbe-42cf-c63a-2b205425495c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WHile we are at it, lets make a decode function"
      ],
      "metadata": {
        "id": "-Sbl6Wk5k73q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  for num in integers:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + \" \"\n",
        "\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9rZJV-Zk_kD",
        "outputId": "8b4c8491-fac2-4e90-a61f-a36661fcfbd2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1, 250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"that movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWrg7pDpmAxI",
        "outputId": "858a86d5-3b6b-4f75-a023-9235be87f3ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "[0.8952339]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0.1833686]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It then gives us a 90 percent accuracy of positivity for the psotivie review and about 20 percent positivity for the negative review. We can also add an if statement that lets it return positive if the accuracy is over 0.5 and negative otherwise"
      ],
      "metadata": {
        "id": "0tuTNaS418HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-tC2RfGr1_q"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}