{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdad946a-5813-4253-be54-a4ae9682ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db1efca-6b33-489a-aee6-bbcd9813abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')[:80_000]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd3e3440-3c78-40ec-80ec-6a14ed5968f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shapes of X=(80000, 28) y=(80000,), Fraud Cases=196'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['Time', 'Amount', 'Class']).values\n",
    "y = df['Class'].values\n",
    "f\"Shapes of X={X.shape} y={y.shape}, Fraud Cases={y.sum()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4a8ff6b-7f6d-4993-b052-826f5e1d6d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# We can increase the weight of each class i.e for non fraud cases 0, we  assign a weight of 1 and for fraud cases 1, we give it double the weight, the idea is that we might have more fraud cases selected\\\n",
    "mod = LogisticRegression(class_weight={0: 1, 1:2}, max_iter=1000)\n",
    "mod.fit(X, y).predict(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd458a0b-fc7e-4a83-8269-fe2522e77b6a",
   "metadata": {},
   "source": [
    "## We can now find the best value for the class weight using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ed80725-4ecd-4583-a3cd-a9882fa5733a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_grid={'class_weight': [{0: 1, 1: v} for v in range(1,4)]},\n",
    "    cv=4,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cf0bfec-fbf4-4120-ac9b-99b4f3879756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.703079</td>\n",
       "      <td>0.467111</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>{0: 1, 1: 1}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1}}</td>\n",
       "      <td>0.99405</td>\n",
       "      <td>0.99835</td>\n",
       "      <td>0.99945</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>0.997413</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.567236</td>\n",
       "      <td>0.616730</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>{0: 1, 1: 2}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2}}</td>\n",
       "      <td>0.99020</td>\n",
       "      <td>0.99840</td>\n",
       "      <td>0.99960</td>\n",
       "      <td>0.99805</td>\n",
       "      <td>0.996563</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.919939</td>\n",
       "      <td>0.121488</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 3}}</td>\n",
       "      <td>0.98730</td>\n",
       "      <td>0.99845</td>\n",
       "      <td>0.99960</td>\n",
       "      <td>0.99815</td>\n",
       "      <td>0.995875</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.703079      0.467111         0.009674        0.000585   \n",
       "1       2.567236      0.616730         0.011410        0.003249   \n",
       "2       1.919939      0.121488         0.004998        0.002119   \n",
       "\n",
       "  param_class_weight                          params  split0_test_score  \\\n",
       "0       {0: 1, 1: 1}  {'class_weight': {0: 1, 1: 1}}            0.99405   \n",
       "1       {0: 1, 1: 2}  {'class_weight': {0: 1, 1: 2}}            0.99020   \n",
       "2       {0: 1, 1: 3}  {'class_weight': {0: 1, 1: 3}}            0.98730   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0            0.99835            0.99945            0.99780         0.997413   \n",
       "1            0.99840            0.99960            0.99805         0.996563   \n",
       "2            0.99845            0.99960            0.99815         0.995875   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.002030                1  \n",
       "1        0.003718                2  \n",
       "2        0.004980                3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4cdd2-f592-4233-a161-f955ff7f8ea5",
   "metadata": {},
   "source": [
    "## The score comems from our model, we can have the LogisticRegression().score and with lr = LogisticRegression(); ??lr.score we can see the implementation of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "923c76ba-e306-4866-b2e3-401275a4a77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"\n",
       "        Return the mean accuracy on the given test data and labels.\n",
       "\n",
       "        In multi-label classification, this is the subset accuracy\n",
       "        which is a harsh metric since you require for each sample that\n",
       "        each label set be correctly predicted.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        X : array-like of shape (n_samples, n_features)\n",
       "            Test samples.\n",
       "\n",
       "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
       "            True labels for `X`.\n",
       "\n",
       "        sample_weight : array-like of shape (n_samples,), default=None\n",
       "            Sample weights.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        score : float\n",
       "            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\sklearn\\base.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "??lr.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7564838-3e26-431d-b745-43dbf78c5a7b",
   "metadata": {},
   "source": [
    "## We can see that it uses the accuracy_score unless we specify otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c54cce02-40e5-4c65-a138-e07a76945ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb34cf5-7a93-4b4c-be2d-f807cf54ee98",
   "metadata": {},
   "source": [
    "## precision and recall both measure different things, what recall will tell us is that it will tell us did we get all the fraud cases and precision is saying: given that we predict fraud, how accurate are we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9198f31c-5345-4b3e-9216-7c0673485c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5918367346938775"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision_score(y, grid.predict(X))\n",
    "recall_score(y, grid.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3521cec-af7a-4815-a98c-c39276b217e6",
   "metadata": {},
   "source": [
    "## Now, lets add precision and recall to the grid search: we have to tell the gridsearch to select the best model based on one of  the scores which is the refit property\n",
    "### Higher cross validation would make our program take longer to run but it would have more accuracy when it comes out\n",
    "### We can also use a numpy linear space with a dimension to increase the effect of the class weight. And by setting the class weight to a higher value, we are telling the algorithm to focus on the fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "223e78d8-c1ca-4198-a891-4aa037aa9841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit=&#x27;precision&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;precision&#x27;: make_scorer(precision_score),\n",
       "                      &#x27;recall_score&#x27;: make_scorer(recall_score)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit=&#x27;precision&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;precision&#x27;: make_scorer(precision_score),\n",
       "                      &#x27;recall_score&#x27;: make_scorer(recall_score)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit='precision', return_train_score=True,\n",
       "             scoring={'precision': make_scorer(precision_score),\n",
       "                      'recall_score': make_scorer(recall_score)})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},\n",
    "    scoring={'precision': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)},\n",
    "    refit='precision',\n",
    "    return_train_score=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e647a4bf-cc37-4159-80d9-d699caec3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>split3_test_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_recall_score</th>\n",
       "      <th>split3_train_recall_score</th>\n",
       "      <th>split4_train_recall_score</th>\n",
       "      <th>split5_train_recall_score</th>\n",
       "      <th>split6_train_recall_score</th>\n",
       "      <th>split7_train_recall_score</th>\n",
       "      <th>split8_train_recall_score</th>\n",
       "      <th>split9_train_recall_score</th>\n",
       "      <th>mean_train_recall_score</th>\n",
       "      <th>std_train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.671474</td>\n",
       "      <td>0.410574</td>\n",
       "      <td>0.032869</td>\n",
       "      <td>0.025420</td>\n",
       "      <td>{0: 1, 1: 1.0}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1.0}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.548023</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.612185</td>\n",
       "      <td>0.054733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.114547</td>\n",
       "      <td>0.244071</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>{0: 1, 1: 1.6551724137931034}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1.6551724137931034}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683616</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.698864</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.680239</td>\n",
       "      <td>0.050286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.137518</td>\n",
       "      <td>0.413214</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>{0: 1, 1: 2.310344827586207}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2.310344827586207}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>0.683616</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>0.698864</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.744318</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.724454</td>\n",
       "      <td>0.043881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.415025</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.028733</td>\n",
       "      <td>0.015296</td>\n",
       "      <td>{0: 1, 1: 2.9655172413793105}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2.9655172413793105}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785311</td>\n",
       "      <td>0.706215</td>\n",
       "      <td>0.744318</td>\n",
       "      <td>0.732955</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>0.749978</td>\n",
       "      <td>0.039589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.030541</td>\n",
       "      <td>0.470301</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>{0: 1, 1: 3.6206896551724137}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 3.6206896551724137}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.744318</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.761364</td>\n",
       "      <td>0.771498</td>\n",
       "      <td>0.037959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.163898</td>\n",
       "      <td>0.404780</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>{0: 1, 1: 4.275862068965517}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 4.275862068965517}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.768362</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.792485</td>\n",
       "      <td>0.029289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.147438</td>\n",
       "      <td>0.188007</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>{0: 1, 1: 4.931034482758621}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 4.931034482758621}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.812327</td>\n",
       "      <td>0.021063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.925633</td>\n",
       "      <td>0.406611</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>{0: 1, 1: 5.586206896551724}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 5.586206896551724}}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.827080</td>\n",
       "      <td>0.017190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.058873</td>\n",
       "      <td>0.328785</td>\n",
       "      <td>0.026412</td>\n",
       "      <td>0.016289</td>\n",
       "      <td>{0: 1, 1: 6.241379310344827}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 6.241379310344827}}</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.838431</td>\n",
       "      <td>0.013974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.148276</td>\n",
       "      <td>0.526671</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>{0: 1, 1: 6.896551724137931}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 6.896551724137931}}</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.844665</td>\n",
       "      <td>0.012015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.058091</td>\n",
       "      <td>0.334820</td>\n",
       "      <td>0.022297</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>{0: 1, 1: 7.551724137931034}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 7.551724137931034}}</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.848064</td>\n",
       "      <td>0.010502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.216132</td>\n",
       "      <td>0.508935</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>{0: 1, 1: 8.206896551724139}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 8.206896551724139}}</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.852038</td>\n",
       "      <td>0.009997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.261937</td>\n",
       "      <td>0.400465</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>{0: 1, 1: 8.862068965517242}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 8.862068965517242}}</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.854305</td>\n",
       "      <td>0.011103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.196642</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>{0: 1, 1: 9.517241379310345}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 9.517241379310345}}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.855441</td>\n",
       "      <td>0.011414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.866511</td>\n",
       "      <td>0.377627</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>{0: 1, 1: 10.172413793103448}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 10.172413793103448}}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.856009</td>\n",
       "      <td>0.011097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.971525</td>\n",
       "      <td>0.488426</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>{0: 1, 1: 10.827586206896552}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 10.827586206896552}}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.856577</td>\n",
       "      <td>0.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.039043</td>\n",
       "      <td>0.229548</td>\n",
       "      <td>0.035740</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>{0: 1, 1: 11.482758620689655}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 11.482758620689655}}</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.880682</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.859415</td>\n",
       "      <td>0.011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.376841</td>\n",
       "      <td>0.416583</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>{0: 1, 1: 12.137931034482758}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 12.137931034482758}}</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875706</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.880682</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.862811</td>\n",
       "      <td>0.011843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.147866</td>\n",
       "      <td>0.284974</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>{0: 1, 1: 12.793103448275861}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 12.793103448275861}}</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.864513</td>\n",
       "      <td>0.012530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.593143</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.027368</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>{0: 1, 1: 13.448275862068964}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 13.448275862068964}}</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.866214</td>\n",
       "      <td>0.010798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.782857</td>\n",
       "      <td>0.585251</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>{0: 1, 1: 14.103448275862068}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 14.103448275862068}}</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.866782</td>\n",
       "      <td>0.011092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.351625</td>\n",
       "      <td>0.459867</td>\n",
       "      <td>0.020544</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>{0: 1, 1: 14.758620689655173}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 14.758620689655173}}</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.010509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.321507</td>\n",
       "      <td>0.188062</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>{0: 1, 1: 15.413793103448276}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 15.413793103448276}}</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.867915</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.892576</td>\n",
       "      <td>0.415681</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>{0: 1, 1: 16.06896551724138}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 16.06896551724138}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.010036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.431800</td>\n",
       "      <td>0.425758</td>\n",
       "      <td>0.027081</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>{0: 1, 1: 16.724137931034484}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 16.724137931034484}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.858757</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.010036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.921849</td>\n",
       "      <td>0.350160</td>\n",
       "      <td>0.025344</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>{0: 1, 1: 17.379310344827587}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 17.379310344827587}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.869617</td>\n",
       "      <td>0.009789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.179597</td>\n",
       "      <td>0.501364</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>{0: 1, 1: 18.03448275862069}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 18.03448275862069}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.892045</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.870185</td>\n",
       "      <td>0.010851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.239039</td>\n",
       "      <td>0.364495</td>\n",
       "      <td>0.023078</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>{0: 1, 1: 18.689655172413794}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 18.689655172413794}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.897727</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.872458</td>\n",
       "      <td>0.011025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.218648</td>\n",
       "      <td>0.394670</td>\n",
       "      <td>0.020782</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>{0: 1, 1: 19.344827586206897}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 19.344827586206897}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.897727</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.872458</td>\n",
       "      <td>0.011025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.736888</td>\n",
       "      <td>0.368515</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>{0: 1, 1: 20.0}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 20.0}}</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.869318</td>\n",
       "      <td>0.897727</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.873588</td>\n",
       "      <td>0.010104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        3.671474      0.410574         0.032869        0.025420   \n",
       "1        3.114547      0.244071         0.023305        0.006509   \n",
       "2        3.137518      0.413214         0.022142        0.004209   \n",
       "3        3.415025      0.312025         0.028733        0.015296   \n",
       "4        3.030541      0.470301         0.020457        0.003053   \n",
       "5        3.163898      0.404780         0.021819        0.004870   \n",
       "6        3.147438      0.188007         0.022594        0.004060   \n",
       "7        2.925633      0.406611         0.020327        0.003530   \n",
       "8        3.058873      0.328785         0.026412        0.016289   \n",
       "9        3.148276      0.526671         0.020580        0.003165   \n",
       "10       3.058091      0.334820         0.022297        0.006981   \n",
       "11       3.216132      0.508935         0.024103        0.008191   \n",
       "12       3.261937      0.400465         0.022597        0.004352   \n",
       "13       3.196642      0.526300         0.022516        0.010870   \n",
       "14       2.866511      0.377627         0.025618        0.015774   \n",
       "15       2.971525      0.488426         0.023313        0.008999   \n",
       "16       3.039043      0.229548         0.035740        0.020834   \n",
       "17       3.376841      0.416583         0.031272        0.013728   \n",
       "18       3.147866      0.284974         0.024862        0.011161   \n",
       "19       3.593143      0.871959         0.027368        0.020877   \n",
       "20       3.782857      0.585251         0.020376        0.002690   \n",
       "21       3.351625      0.459867         0.020544        0.006042   \n",
       "22       3.321507      0.188062         0.025982        0.012146   \n",
       "23       2.892576      0.415681         0.029976        0.018647   \n",
       "24       3.431800      0.425758         0.027081        0.011950   \n",
       "25       2.921849      0.350160         0.025344        0.010714   \n",
       "26       3.179597      0.501364         0.023954        0.008276   \n",
       "27       3.239039      0.364495         0.023078        0.009357   \n",
       "28       3.218648      0.394670         0.020782        0.002883   \n",
       "29       2.736888      0.368515         0.015546        0.005604   \n",
       "\n",
       "               param_class_weight  \\\n",
       "0                  {0: 1, 1: 1.0}   \n",
       "1   {0: 1, 1: 1.6551724137931034}   \n",
       "2    {0: 1, 1: 2.310344827586207}   \n",
       "3   {0: 1, 1: 2.9655172413793105}   \n",
       "4   {0: 1, 1: 3.6206896551724137}   \n",
       "5    {0: 1, 1: 4.275862068965517}   \n",
       "6    {0: 1, 1: 4.931034482758621}   \n",
       "7    {0: 1, 1: 5.586206896551724}   \n",
       "8    {0: 1, 1: 6.241379310344827}   \n",
       "9    {0: 1, 1: 6.896551724137931}   \n",
       "10   {0: 1, 1: 7.551724137931034}   \n",
       "11   {0: 1, 1: 8.206896551724139}   \n",
       "12   {0: 1, 1: 8.862068965517242}   \n",
       "13   {0: 1, 1: 9.517241379310345}   \n",
       "14  {0: 1, 1: 10.172413793103448}   \n",
       "15  {0: 1, 1: 10.827586206896552}   \n",
       "16  {0: 1, 1: 11.482758620689655}   \n",
       "17  {0: 1, 1: 12.137931034482758}   \n",
       "18  {0: 1, 1: 12.793103448275861}   \n",
       "19  {0: 1, 1: 13.448275862068964}   \n",
       "20  {0: 1, 1: 14.103448275862068}   \n",
       "21  {0: 1, 1: 14.758620689655173}   \n",
       "22  {0: 1, 1: 15.413793103448276}   \n",
       "23   {0: 1, 1: 16.06896551724138}   \n",
       "24  {0: 1, 1: 16.724137931034484}   \n",
       "25  {0: 1, 1: 17.379310344827587}   \n",
       "26   {0: 1, 1: 18.03448275862069}   \n",
       "27  {0: 1, 1: 18.689655172413794}   \n",
       "28  {0: 1, 1: 19.344827586206897}   \n",
       "29                {0: 1, 1: 20.0}   \n",
       "\n",
       "                                             params  split0_test_precision  \\\n",
       "0                  {'class_weight': {0: 1, 1: 1.0}}               1.000000   \n",
       "1   {'class_weight': {0: 1, 1: 1.6551724137931034}}               1.000000   \n",
       "2    {'class_weight': {0: 1, 1: 2.310344827586207}}               1.000000   \n",
       "3   {'class_weight': {0: 1, 1: 2.9655172413793105}}               1.000000   \n",
       "4   {'class_weight': {0: 1, 1: 3.6206896551724137}}               1.000000   \n",
       "5    {'class_weight': {0: 1, 1: 4.275862068965517}}               1.000000   \n",
       "6    {'class_weight': {0: 1, 1: 4.931034482758621}}               1.000000   \n",
       "7    {'class_weight': {0: 1, 1: 5.586206896551724}}               1.000000   \n",
       "8    {'class_weight': {0: 1, 1: 6.241379310344827}}               0.944444   \n",
       "9    {'class_weight': {0: 1, 1: 6.896551724137931}}               0.944444   \n",
       "10   {'class_weight': {0: 1, 1: 7.551724137931034}}               0.944444   \n",
       "11   {'class_weight': {0: 1, 1: 8.206896551724139}}               0.944444   \n",
       "12   {'class_weight': {0: 1, 1: 8.862068965517242}}               0.944444   \n",
       "13   {'class_weight': {0: 1, 1: 9.517241379310345}}               0.894737   \n",
       "14  {'class_weight': {0: 1, 1: 10.172413793103448}}               0.850000   \n",
       "15  {'class_weight': {0: 1, 1: 10.827586206896552}}               0.850000   \n",
       "16  {'class_weight': {0: 1, 1: 11.482758620689655}}               0.857143   \n",
       "17  {'class_weight': {0: 1, 1: 12.137931034482758}}               0.857143   \n",
       "18  {'class_weight': {0: 1, 1: 12.793103448275861}}               0.857143   \n",
       "19  {'class_weight': {0: 1, 1: 13.448275862068964}}               0.818182   \n",
       "20  {'class_weight': {0: 1, 1: 14.103448275862068}}               0.818182   \n",
       "21  {'class_weight': {0: 1, 1: 14.758620689655173}}               0.818182   \n",
       "22  {'class_weight': {0: 1, 1: 15.413793103448276}}               0.818182   \n",
       "23   {'class_weight': {0: 1, 1: 16.06896551724138}}               0.782609   \n",
       "24  {'class_weight': {0: 1, 1: 16.724137931034484}}               0.782609   \n",
       "25  {'class_weight': {0: 1, 1: 17.379310344827587}}               0.782609   \n",
       "26   {'class_weight': {0: 1, 1: 18.03448275862069}}               0.782609   \n",
       "27  {'class_weight': {0: 1, 1: 18.689655172413794}}               0.782609   \n",
       "28  {'class_weight': {0: 1, 1: 19.344827586206897}}               0.782609   \n",
       "29                {'class_weight': {0: 1, 1: 20.0}}               0.782609   \n",
       "\n",
       "    split1_test_precision  split2_test_precision  split3_test_precision  ...  \\\n",
       "0                0.463415               0.583333               1.000000  ...   \n",
       "1                0.463415               0.583333               1.000000  ...   \n",
       "2                0.463415               0.583333               1.000000  ...   \n",
       "3                0.452381               0.583333               1.000000  ...   \n",
       "4                0.452381               0.583333               1.000000  ...   \n",
       "5                0.452381               0.583333               1.000000  ...   \n",
       "6                0.452381               0.583333               1.000000  ...   \n",
       "7                0.452381               0.583333               1.000000  ...   \n",
       "8                0.452381               0.583333               0.947368  ...   \n",
       "9                0.452381               0.583333               0.947368  ...   \n",
       "10               0.452381               0.583333               0.947368  ...   \n",
       "11               0.452381               0.583333               0.947368  ...   \n",
       "12               0.441860               0.583333               0.947368  ...   \n",
       "13               0.431818               0.560000               0.947368  ...   \n",
       "14               0.431818               0.560000               0.947368  ...   \n",
       "15               0.431818               0.560000               0.947368  ...   \n",
       "16               0.431818               0.560000               0.947368  ...   \n",
       "17               0.431818               0.576923               0.947368  ...   \n",
       "18               0.413043               0.576923               0.947368  ...   \n",
       "19               0.413043               0.576923               0.947368  ...   \n",
       "20               0.413043               0.576923               0.947368  ...   \n",
       "21               0.404255               0.576923               0.947368  ...   \n",
       "22               0.387755               0.576923               0.947368  ...   \n",
       "23               0.380000               0.576923               0.947368  ...   \n",
       "24               0.380000               0.555556               0.947368  ...   \n",
       "25               0.380000               0.555556               0.947368  ...   \n",
       "26               0.365385               0.535714               0.947368  ...   \n",
       "27               0.345455               0.535714               0.947368  ...   \n",
       "28               0.345455               0.535714               0.947368  ...   \n",
       "29               0.339286               0.535714               0.947368  ...   \n",
       "\n",
       "    split2_train_recall_score  split3_train_recall_score  \\\n",
       "0                    0.627119                   0.548023   \n",
       "1                    0.683616                   0.627119   \n",
       "2                    0.740113                   0.683616   \n",
       "3                    0.785311                   0.706215   \n",
       "4                    0.824859                   0.740113   \n",
       "5                    0.841808                   0.768362   \n",
       "6                    0.847458                   0.802260   \n",
       "7                    0.847458                   0.813559   \n",
       "8                    0.847458                   0.824859   \n",
       "9                    0.853107                   0.830508   \n",
       "10                   0.858757                   0.841808   \n",
       "11                   0.858757                   0.841808   \n",
       "12                   0.870056                   0.841808   \n",
       "13                   0.870056                   0.841808   \n",
       "14                   0.870056                   0.841808   \n",
       "15                   0.870056                   0.841808   \n",
       "16                   0.870056                   0.841808   \n",
       "17                   0.875706                   0.853107   \n",
       "18                   0.881356                   0.853107   \n",
       "19                   0.881356                   0.858757   \n",
       "20                   0.881356                   0.858757   \n",
       "21                   0.881356                   0.858757   \n",
       "22                   0.881356                   0.858757   \n",
       "23                   0.881356                   0.858757   \n",
       "24                   0.881356                   0.858757   \n",
       "25                   0.881356                   0.864407   \n",
       "26                   0.881356                   0.864407   \n",
       "27                   0.881356                   0.864407   \n",
       "28                   0.881356                   0.864407   \n",
       "29                   0.881356                   0.864407   \n",
       "\n",
       "    split4_train_recall_score  split5_train_recall_score  \\\n",
       "0                    0.573864                   0.573864   \n",
       "1                    0.670455                   0.647727   \n",
       "2                    0.710227                   0.698864   \n",
       "3                    0.744318                   0.732955   \n",
       "4                    0.755682                   0.744318   \n",
       "5                    0.778409                   0.789773   \n",
       "6                    0.789773                   0.818182   \n",
       "7                    0.829545                   0.829545   \n",
       "8                    0.846591                   0.835227   \n",
       "9                    0.846591                   0.835227   \n",
       "10                   0.852273                   0.840909   \n",
       "11                   0.857955                   0.846591   \n",
       "12                   0.857955                   0.846591   \n",
       "13                   0.857955                   0.846591   \n",
       "14                   0.857955                   0.852273   \n",
       "15                   0.857955                   0.852273   \n",
       "16                   0.863636                   0.852273   \n",
       "17                   0.869318                   0.852273   \n",
       "18                   0.869318                   0.852273   \n",
       "19                   0.869318                   0.857955   \n",
       "20                   0.875000                   0.857955   \n",
       "21                   0.875000                   0.857955   \n",
       "22                   0.875000                   0.863636   \n",
       "23                   0.875000                   0.863636   \n",
       "24                   0.875000                   0.863636   \n",
       "25                   0.875000                   0.863636   \n",
       "26                   0.875000                   0.863636   \n",
       "27                   0.875000                   0.875000   \n",
       "28                   0.875000                   0.875000   \n",
       "29                   0.875000                   0.875000   \n",
       "\n",
       "    split6_train_recall_score  split7_train_recall_score  \\\n",
       "0                    0.562500                   0.613636   \n",
       "1                    0.630682                   0.687500   \n",
       "2                    0.687500                   0.715909   \n",
       "3                    0.715909                   0.755682   \n",
       "4                    0.727273                   0.778409   \n",
       "5                    0.772727                   0.789773   \n",
       "6                    0.801136                   0.801136   \n",
       "7                    0.812500                   0.806818   \n",
       "8                    0.823864                   0.829545   \n",
       "9                    0.829545                   0.835227   \n",
       "10                   0.835227                   0.835227   \n",
       "11                   0.835227                   0.846591   \n",
       "12                   0.835227                   0.852273   \n",
       "13                   0.835227                   0.863636   \n",
       "14                   0.835227                   0.863636   \n",
       "15                   0.835227                   0.863636   \n",
       "16                   0.840909                   0.863636   \n",
       "17                   0.840909                   0.869318   \n",
       "18                   0.846591                   0.869318   \n",
       "19                   0.852273                   0.869318   \n",
       "20                   0.852273                   0.869318   \n",
       "21                   0.852273                   0.869318   \n",
       "22                   0.852273                   0.869318   \n",
       "23                   0.852273                   0.869318   \n",
       "24                   0.852273                   0.869318   \n",
       "25                   0.852273                   0.869318   \n",
       "26                   0.852273                   0.869318   \n",
       "27                   0.857955                   0.869318   \n",
       "28                   0.857955                   0.869318   \n",
       "29                   0.857955                   0.869318   \n",
       "\n",
       "    split8_train_recall_score  split9_train_recall_score  \\\n",
       "0                    0.636364                   0.607955   \n",
       "1                    0.698864                   0.687500   \n",
       "2                    0.744318                   0.727273   \n",
       "3                    0.772727                   0.738636   \n",
       "4                    0.784091                   0.761364   \n",
       "5                    0.795455                   0.772727   \n",
       "6                    0.818182                   0.789773   \n",
       "7                    0.835227                   0.818182   \n",
       "8                    0.857955                   0.829545   \n",
       "9                    0.863636                   0.846591   \n",
       "10                   0.863636                   0.846591   \n",
       "11                   0.869318                   0.852273   \n",
       "12                   0.869318                   0.857955   \n",
       "13                   0.869318                   0.857955   \n",
       "14                   0.869318                   0.857955   \n",
       "15                   0.875000                   0.857955   \n",
       "16                   0.880682                   0.863636   \n",
       "17                   0.880682                   0.863636   \n",
       "18                   0.886364                   0.863636   \n",
       "19                   0.886364                   0.863636   \n",
       "20                   0.886364                   0.863636   \n",
       "21                   0.886364                   0.863636   \n",
       "22                   0.886364                   0.863636   \n",
       "23                   0.886364                   0.869318   \n",
       "24                   0.886364                   0.869318   \n",
       "25                   0.886364                   0.875000   \n",
       "26                   0.892045                   0.875000   \n",
       "27                   0.897727                   0.875000   \n",
       "28                   0.897727                   0.875000   \n",
       "29                   0.897727                   0.875000   \n",
       "\n",
       "    mean_train_recall_score  std_train_recall_score  \n",
       "0                  0.612185                0.054733  \n",
       "1                  0.680239                0.050286  \n",
       "2                  0.724454                0.043881  \n",
       "3                  0.749978                0.039589  \n",
       "4                  0.771498                0.037959  \n",
       "5                  0.792485                0.029289  \n",
       "6                  0.812327                0.021063  \n",
       "7                  0.827080                0.017190  \n",
       "8                  0.838431                0.013974  \n",
       "9                  0.844665                0.012015  \n",
       "10                 0.848064                0.010502  \n",
       "11                 0.852038                0.009997  \n",
       "12                 0.854305                0.011103  \n",
       "13                 0.855441                0.011414  \n",
       "14                 0.856009                0.011097  \n",
       "15                 0.856577                0.011881  \n",
       "16                 0.859415                0.011778  \n",
       "17                 0.862811                0.011843  \n",
       "18                 0.864513                0.012530  \n",
       "19                 0.866214                0.010798  \n",
       "20                 0.866782                0.011092  \n",
       "21                 0.867347                0.010509  \n",
       "22                 0.867915                0.010133  \n",
       "23                 0.868484                0.010036  \n",
       "24                 0.868484                0.010036  \n",
       "25                 0.869617                0.009789  \n",
       "26                 0.870185                0.010851  \n",
       "27                 0.872458                0.011025  \n",
       "28                 0.872458                0.011025  \n",
       "29                 0.873588                0.010104  \n",
       "\n",
       "[30 rows x 56 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09473254-f12e-46e7-9f13-9439d20ebafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24f0add1b70>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "for score in ['mean_test_recall_score', 'mean_test_precision']:\n",
    "    plt.plot([_[1] for _ in df['param_class_weight']],\n",
    "            df[score],\n",
    "            label=score)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa09bf57-3d18-4be0-9341-8c983892fab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.780937\n",
       "1     0.882453\n",
       "2     0.882453\n",
       "3     0.881349\n",
       "4     0.874206\n",
       "5     0.874683\n",
       "6     0.876749\n",
       "7     0.872877\n",
       "8     0.863844\n",
       "9     0.859370\n",
       "10    0.859370\n",
       "11    0.859370\n",
       "12    0.858318\n",
       "13    0.850010\n",
       "14    0.845536\n",
       "15    0.845536\n",
       "16    0.846986\n",
       "17    0.849392\n",
       "18    0.842286\n",
       "19    0.838390\n",
       "20    0.838390\n",
       "21    0.832833\n",
       "22    0.831183\n",
       "23    0.822954\n",
       "24    0.804012\n",
       "25    0.799627\n",
       "26    0.796181\n",
       "27    0.788547\n",
       "28    0.784599\n",
       "29    0.780411\n",
       "Name: mean_test_precision, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_test_precision']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
